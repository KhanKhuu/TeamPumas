In exam, DO NOT LEAVE CANVAS (exam will close if you leave the page)
Questions will be about understanding these algorithms, the data structures we've built
    If there is coding, it will be small (like fix this or add something to make this work)
    The majority will be understanding algorithms


QuickSort
    Tony Hoare - inverntor
    Divide and conquer algorithm (like MergeSort)
        1) Partition array into two regions
        2) Small items moved to the left of the pivot
        3) Large items moved right
        4) After partitioning, repeat the sort on both sides
Common technique: use the first item in the region as a pivot:
    everything less than the piveot goes left, everything else goes right

Outline:
qsort(a, lower, upper):
    if lower < upper
        mid = partition(a, lower, upper) //returns location of the boundary between subregions
        qsort(a, lower, mid)
        qsort(a, mid+1, upper)

Worst case: pivot is the  unique minimum or maximum element
One of L and G has size n - 1 and the other has size 0
The running time is proportional to the sum

Good call-> the sizes of L and G are less than 3/4
Bad call-> L or G has size > 3/4
A call with probability 1/2 -> 1/2 of the numbers produce a good call

For a node of depth i, we expect
    i/2 ancestors are good calls
    The size of input sequence for the current call is at most (3/4)^i/2n
    Expected height of quicksort tree is O(Logn)

Partition
    Check every item once (O)n
Conquer
    Divide data in half O(Log_2n)
Total   Product -> O(nLog_2n)

Same as heapsort/mergesort
    quicksort is generally faster
But there's a catch

What happens if we run quicksort on already sorted?
    IF we start with the first element:
        O(n*n)
            ie n^2
    Because each partition produces a problem of size 0 and one of size n-1
    Number of partitions n each needing time O(n)
    therefore n^2

Is there any way to ensure better performance?
    Hoare: Choose 1st element
    Lomuto: partition around last element

Best Case: Choose a different pivot so that the partitions are equal, then we will see O(nLogn) time

Take 2 positions and choose median (1st, middle, last)
    For an already sorted list, you get the perfect division every time! therefore no n^2 problem for already sorted data
    and it helps with unsorted data, too
        TAKING THE MEDIAN THIS WAY IS GENERALLY THE BEST WAY TO CHOOSE A PIVOT -> Particularly when the data is already sorted
Another option: Randomly choose a pivot
    On average, sorted data is divided evenly
    O(nlogn) time
KEY REQUIREMENT: Pivot choice must take O(1) time

Never guaranteed nLogn time with quicksort (we want it, but it is not possible to guarantee it)
    In fact, there is always a possibilty given random data that quicksort will take n^2 time



Perform the partition using two indices to split S and L and E U G (a similar method can split E U G into E and G

Repeat until J and K cross
    Scan J to the right until finding el >= x
    Scan K to left until el < x
swap elements at indices j and K


Performance of Various Sorting Algorithms

                Worst       Expected        Extra Memory
Insertion       n^2         n^2             O(1)
Merge           nlgn        nlgn            O(n)
Quick           n^2         nlgn            O(1)
Heap            nlgn        nlgn            O(1)


Merge sort has constant O(nlogn)
    Stable
Quick sort has expected O(nLogn) and worst time n^2
    Unstable

Why use quick sort?
    Cache utilization
    processor optimizations
    File IO in relation to large data
        Therefore, slower on paper but faster in reality 

EX Yahoo
    1,000,000 record file
    8KB pages
    100 bytes/record
    Use merge sort to take advantage of disk IO
    
    
Miscellaneous Notes: 
    Y Liang, author of one of the best C++ books according to Prof Elkady

